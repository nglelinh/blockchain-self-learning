---
layout: post
title: "Lecture 02.01: Byzantine Fault Tolerant Consensus - PBFT, Tendermint, HotStuff"
chapter: '02'
order: 2
owner: Blockchain Course Team
lang: vi
categories:
- blockchain-chapter02
---

# Lecture: Byzantine Fault Tolerant Consensus - PBFT, Tendermint, HotStuff

## 1. T·ªïng quan v·ªÅ kh√°i ni·ªám

**Byzantine Fault Tolerant (BFT) consensus** l√† m·ªôt family of protocols cho ph√©p distributed systems ƒë·∫°t ƒë∆∞·ª£c agreement ngay c·∫£ khi m·ªôt s·ªë nodes h√†nh ƒë·ªông maliciously ho·∫∑c arbitrarily faulty. ƒê√¢y l√† foundation cho modern blockchain consensus mechanisms, ƒë·∫∑c bi·ªát trong permissioned v√† high-performance blockchains.

**The Byzantine Generals Problem**:

V·∫•n ƒë·ªÅ n√†y ƒë∆∞·ª£c formalize b·ªüi **Leslie Lamport, Robert Shostak, v√† Marshall Pease** nƒÉm 1982. Story ƒë∆°n gi·∫£n nh∆∞ng profound:

> M·ªôt nh√≥m Byzantine generals bao v√¢y m·ªôt th√†nh ph·ªë. H·ªç ph·∫£i decide: attack ho·∫∑c retreat. C√°c generals communicate qua messengers. M·ªôt s·ªë generals c√≥ th·ªÉ l√† traitors, g·ª≠i conflicting messages. L√†m th·∫ø n√†o ƒë·ªÉ loyal generals ƒë·∫°t ƒë∆∞·ª£c agreement?

**Formal Problem**:
- \( n \) processes (generals)
- \( f \) processes c√≥ th·ªÉ Byzantine (traitors)
- Each process has input value
- Goal: All honest processes agree on same output value

**Key Insight**: Problem ch·ªâ solvable n·∫øu:

\[
n \geq 3f + 1
\]

T·ª©c c·∫ßn √≠t nh·∫•t 2/3 honest nodes!

**Why This Matters for Blockchain**:

Trong blockchain context:
- **Nodes = Generals**: Distributed validators/miners
- **Byzantine Nodes = Attackers**: Malicious validators trying to disrupt
- **Agreement = Consensus**: All honest nodes agree on transaction order
- **Messages = Blocks/Votes**: Communication between nodes

BFT protocols provide **deterministic finality** (kh√¥ng nh∆∞ PoW's probabilistic finality) v√† **high throughput** (thousands of TPS), making them ideal cho enterprise blockchains v√† high-performance public chains.

**Evolution of BFT Protocols**:

**1982**: Byzantine Generals Problem formalized
**1999**: **PBFT (Practical Byzantine Fault Tolerance)** - First practical BFT algorithm
**2014**: **Tendermint** - BFT cho public blockchains
**2018**: **HotStuff** - Simplified BFT v·ªõi linear communication
**2019**: **LibraBFT** (now DiemBFT) - Facebook's blockchain consensus
**2020+**: Various improvements v√† optimizations

**Comparison v·ªõi PoW/PoS**:

| Feature | PoW | PoS | BFT |
|---------|-----|-----|-----|
| Finality | Probabilistic | Probabilistic/Absolute | Absolute |
| Throughput | Low (~7 TPS) | Medium (~1000 TPS) | High (~10k TPS) |
| Energy | Very High | Low | Low |
| Latency | Minutes | Seconds | Sub-second |
| Permissioned | No | No | Yes/No |
| Fault Tolerance | 51% | 33% | 33% |

BFT shines trong scenarios c·∫ßn:
- **Fast finality**: Seconds not minutes
- **High throughput**: Thousands of TPS
- **Deterministic finality**: No chain reorganization
- **Known validator set**: Enterprise/consortium chains

---

## 2. Hi·ªÉu bi·∫øt tr·ª±c quan

### 2.1. Byzantine Generals - The Visual Story

T∆∞·ªüng t∆∞·ª£ng 4 generals bao v√¢y m·ªôt castle:

```
        General A (Commander)
              |
    ----------+-----------
    |         |          |
General B  General C  General D
              
Commander's order: "ATTACK at dawn!"
```

**Scenario 1: No Traitors** ‚úì
```
A sends: ATTACK ‚Üí B, C, D
B receives: ATTACK, forwards to C, D
C receives: ATTACK, forwards to B, D  
D receives: ATTACK, forwards to B, C

All see: 4 votes for ATTACK
Decision: ATTACK unanimously
```

**Scenario 2: Commander is Traitor** ‚úó
```
A sends: ATTACK ‚Üí B
A sends: RETREAT ‚Üí C
A sends: ATTACK ‚Üí D

B, C, D exchange messages:
- B and D heard: ATTACK
- C heard: RETREAT

Without BFT protocol: Conflict! Some attack, some retreat
With BFT protocol: Detect inconsistency, use backup plan
```

**Scenario 3: One Lieutenant is Traitor** ‚úó
```
A sends: ATTACK ‚Üí B, C, D (honest)

B (traitor) forwards:
- To C: "A said RETREAT"
- To D: "A said ATTACK"

C and D confused!

Solution (PBFT approach):
- Require 2f+1 matching messages
- With 4 generals (n=4), tolerate f=1
- Need 3 matching messages
- A's original + D's forward + C's forward = 3 ATTACK
- B's lie isolated ‚Üí ATTACK decided
```

### 2.2. PBFT Three-Phase Protocol - Restaurant Analogy

T∆∞·ªüng t∆∞·ª£ng PBFT nh∆∞ **ordering food at restaurant v·ªõi untrusted waiters**:

**Phase 1: PRE-PREPARE (Order Taking)**
```
Head waiter (primary): Takes customer order
Announces to all waiters: "Table 5 wants Pizza"
Other waiters hear the order
```

**Phase 2: PREPARE (Confirmation)**
```
Each waiter confirms they heard: "I confirm: Table 5 wants Pizza"
Broadcast confirmation to all other waiters
Need 2f+1 confirmations to proceed (quorum)

Why? Some waiters might be lying!
With enough confirmations, truth emerges
```

**Phase 3: COMMIT (Execution)**
```
Once 2f+1 confirmations received:
Each waiter commits: "I will deliver Pizza to Table 5"
Broadcast commit message
Once 2f+1 commits received: Actually deliver pizza

Why second round? Ensure everyone knows everyone else knows
(Common knowledge problem)
```

**Safety**: Customer gets correct order even if some waiters malicious!

### 2.3. View Change - Leader Rotation

**Problem**: N·∫øu primary node crashes ho·∫∑c malicious?

**Solution: View Change** (leader rotation)

```
Normal Operation (View 0):
Primary = Node 0
  0 ‚Üí 1, 2, 3 (PRE-PREPARE)
  All nodes exchange PREPARE
  All nodes exchange COMMIT
  ‚úì Block committed

Primary Fails (View 0):
Primary = Node 0 (crashes!)
  0 ‚Üí X (no PRE-PREPARE)
  Timeout!
  
View Change (View 1):
Nodes detect timeout
New primary = Node 1 (round-robin)
  Resume from last checkpoint
  Continue operation
```

**Analogy**: Gi·ªëng nh∆∞ meeting v·ªõi absent chair:
- Vice-chair takes over
- Meeting continues
- No single point of failure

### 2.4. Quorum Intersection - The Key Insight

**Why need 2f+1 votes? Why not simple majority?**

**Key Property**: Any two quorums MUST overlap in at least f+1 nodes

```
Total nodes: n = 3f + 1 = 10 (f = 3)
Quorum size: 2f + 1 = 7

Quorum 1: Nodes {0,1,2,3,4,5,6}
Quorum 2: Nodes {3,4,5,6,7,8,9}

Intersection: {3,4,5,6} = 4 nodes

At least 1 node (4 - 3 = 1) must be honest!
```

**Guarantee**: Kh√¥ng th·ªÉ c√≥ hai conflicting decisions v·ªõi quorum votes!

```
Decision A gets 7 votes (Quorum 1)
Decision B gets 7 votes (Quorum 2)

Impossible! They share 4 nodes.
If one is honest, it can't vote for both.
Contradiction!
```

This mathematical property ensures **safety** (no conflicting commits).

---

## 3. N·ªÅn t·∫£ng k·ªπ thu·∫≠t

### 3.1. PBFT Algorithm - Detailed Protocol

**System Model**:
- \( n = 3f + 1 \) replicas
- Asynchronous network v·ªõi eventual delivery
- Client sends request to primary
- Primary broadcasts to all replicas

**Message Types**:

1. **REQUEST**: Client ‚Üí Primary
   ```
   <REQUEST, operation, timestamp, client_id>
   ```

2. **PRE-PREPARE**: Primary ‚Üí Replicas
   ```
   <PRE-PREPARE, view, sequence, digest(request)>_primary_sig
   ```

3. **PREPARE**: Replica ‚Üí All Replicas
   ```
   <PREPARE, view, sequence, digest, replica_id>_replica_sig
   ```

4. **COMMIT**: Replica ‚Üí All Replicas
   ```
   <COMMIT, view, sequence, digest, replica_id>_replica_sig
   ```

5. **REPLY**: Replicas ‚Üí Client
   ```
   <REPLY, view, timestamp, client_id, replica_id, result>_replica_sig
   ```

**Protocol Flow**:

**Phase 0: Request**
```python
client.send_request(operation):
    request = REQUEST(operation, timestamp, client_id)
    send_to_primary(request)
```

**Phase 1: Pre-Prepare**
```python
primary.receive_request(request):
    if is_valid(request):
        sequence = next_sequence_number()
        digest = hash(request)
        
        pre_prepare = PRE_PREPARE(view, sequence, digest)
        sign(pre_prepare, primary_key)
        
        broadcast_to_replicas(pre_prepare)
        log(pre_prepare)
```

**Phase 2: Prepare**
```python
replica.receive_pre_prepare(pre_prepare):
    if is_valid(pre_prepare) and not_duplicate(pre_prepare):
        prepare = PREPARE(view, sequence, digest, replica_id)
        sign(prepare, replica_key)
        
        broadcast_to_replicas(prepare)
        log(prepare)
        
        # Wait for 2f matching PREPARE messages
        if count_matching_prepares() >= 2*f:
            enter_prepared_state(sequence)
```

**Phase 3: Commit**
```python
replica.enter_prepared_state(sequence):
    commit = COMMIT(view, sequence, digest, replica_id)
    sign(commit, replica_key)
    
    broadcast_to_replicas(commit)
    log(commit)
    
    # Wait for 2f+1 matching COMMIT messages
    if count_matching_commits() >= 2*f + 1:
        execute_operation(sequence)
        send_reply_to_client()
```

**Prepared Certificate**:

Replica enters "prepared" state when it has:
- 1 valid PRE-PREPARE from primary
- 2f matching PREPARE messages from different replicas

**Committed Certificate**:

Replica enters "committed" state when it has:
- Prepared certificate +
- 2f+1 matching COMMIT messages

**Correctness Guarantees**:

**Safety**: N·∫øu m·ªôt honest replica commits operation at sequence \( n \), kh√¥ng c√≥ honest replica n√†o commit different operation at sequence \( n \).

**Liveness**: Eventually, all requests t·ª´ correct clients ƒë∆∞·ª£c executed.

### 3.2. Tendermint Consensus

**Tendermint** (2014, Jae Kwon) adapts BFT cho public blockchains v·ªõi **rotating proposer** v√† **instant finality**.

**Key Innovations**:
1. **Weighted voting**: Validators c√≥ different voting power (PoS-based)
2. **Immediate finality**: Blocks finalized immediately (no probabilistic finality)
3. **Application agnostic**: Consensus layer separate from application

**Consensus Rounds**:

```
Round Structure:
1. PROPOSE: Proposer broadcasts block
2. PREVOTE: Validators vote on proposal
3. PRECOMMIT: Validators commit to vote
4. COMMIT: Block finalized if 2/3+ precommit
```

**State Machine**:

```
        NewHeight
            ‚Üì
         Propose ‚Üê‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
            ‚Üì           ‚îÇ
         Prevote        ‚îÇ
            ‚Üì           ‚îÇ
    (wait for 2/3)     ‚îÇ
            ‚Üì           ‚îÇ
        Precommit      ‚îÇ
            ‚Üì           ‚îÇ
    (wait for 2/3)     ‚îÇ
            ‚Üì           ‚îÇ
         Commit         ‚îÇ
            ‚Üì           ‚îÇ
            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Detailed Algorithm**:

```python
class TendermintConsensus:
    def __init__(self, validators):
        self.validators = validators  # Weighted validator set
        self.height = 0
        self.round = 0
        self.step = "propose"
        self.locked_value = None
        self.locked_round = -1
        self.valid_value = None
        self.valid_round = -1
    
    def propose_step(self):
        """Proposer broadcasts block proposal"""
        if is_proposer(self.height, self.round):
            proposal = create_proposal(
                height=self.height,
                round=self.round,
                value=self.valid_value or select_value(),
                valid_round=self.valid_round
            )
            broadcast(proposal)
        
        schedule_timeout(TimeoutPropose)
        self.step = "prevote"
    
    def prevote_step(self):
        """Validators prevote on proposal"""
        proposal = get_proposal(self.height, self.round)
        
        if proposal and is_valid(proposal):
            if self.locked_value is None or \
               self.locked_value == proposal.value or \
               proposal.valid_round > self.locked_round:
                prevote = create_prevote(
                    height=self.height,
                    round=self.round,
                    value=proposal.value
                )
            else:
                # Locked on different value
                prevote = create_prevote(nil)
        else:
            prevote = create_prevote(nil)
        
        broadcast(prevote)
        schedule_timeout(TimeoutPrevote)
        self.step = "precommit"
    
    def precommit_step(self):
        """Validators precommit if 2/3+ prevoted"""
        prevotes = get_prevotes(self.height, self.round)
        
        if has_supermajority(prevotes, value):
            # Received 2/3+ prevotes for same value
            if self.locked_round < self.round:
                self.locked_value = value
                self.locked_round = self.round
            
            self.valid_value = value
            self.valid_round = self.round
            
            precommit = create_precommit(
                height=self.height,
                round=self.round,
                value=value
            )
        else:
            precommit = create_precommit(nil)
        
        broadcast(precommit)
        schedule_timeout(TimeoutPrecommit)
    
    def commit_step(self):
        """Commit if 2/3+ precommitted same value"""
        precommits = get_precommits(self.height, self.round)
        
        if has_supermajority(precommits, value):
            # Finalize block
            finalize_block(value)
            
            # Move to next height
            self.height += 1
            self.round = 0
            self.locked_value = None
            self.locked_round = -1
            self.valid_value = None
            self.valid_round = -1
            
            self.step = "propose"
            self.propose_step()
    
    def timeout_propose(self):
        """Timeout waiting for proposal"""
        prevote = create_prevote(nil)
        broadcast(prevote)
        self.step = "prevote"
    
    def timeout_prevote(self):
        """Timeout waiting for prevotes"""
        precommit = create_precommit(nil)
        broadcast(precommit)
        self.step = "precommit"
    
    def timeout_precommit(self):
        """Timeout waiting for precommits - go to next round"""
        self.round += 1
        self.step = "propose"
        self.propose_step()
```

**Locking Mechanism**:

Tendermint uses **locking** ƒë·ªÉ prevent safety violations:
- Validator locks on value sau khi prevoting
- Cannot prevote for different value unless higher valid round
- Ensures validators don't flip-flop between conflicting blocks

**Accountability**:

Tendermint provides **fork accountability**: n·∫øu 1/3+ validators misbehave, c√≥ cryptographic proof c·ªßa violation.

### 3.3. HotStuff - Linear Communication

**HotStuff** (2018, VMware Research) simplifies BFT v·ªõi **linear message complexity** (O(n) thay v√¨ O(n¬≤)).

**Key Innovation: Three-Chain Commit Rule**

```
Block chain:
b1 ‚Üê b2 ‚Üê b3 ‚Üê b4

Commit rule:
If b1, b2, b3 consecutive v√† each has 2/3+ votes
‚Üí b1 is committed (finalized)

Three-phase commit compressed into chain structure!
```

**Protocol Phases**:

**Phase 1: PREPARE**
```
Leader proposes block b
Replicas vote: PREPARE(b)
If 2/3+ votes ‚Üí QC(PREPARE, b)
```

**Phase 2: PRE-COMMIT**
```
Leader broadcasts QC(PREPARE, b)
Replicas vote: PRE-COMMIT(b)
If 2/3+ votes ‚Üí QC(PRE-COMMIT, b)
```

**Phase 3: COMMIT**
```
Leader broadcasts QC(PRE-COMMIT, b)
Replicas vote: COMMIT(b)
If 2/3+ votes ‚Üí QC(COMMIT, b)
```

**Phase 4: DECIDE**
```
Leader broadcasts QC(COMMIT, b)
Replicas finalize b
```

**Chained HotStuff Optimization**:

Thay v√¨ explicit 4 phases, chain consecutive blocks:

```
Block k:     PREPARE phase for k, PRE-COMMIT for k-1
Block k+1:   PREPARE for k+1, PRE-COMMIT for k, COMMIT for k-1
Block k+2:   PREPARE for k+2, PRE-COMMIT for k+1, COMMIT for k, DECIDE k-1

Result: Block k-1 finalized when k+2 produced!
2-block latency for finality
```

**Algorithm**:

```python
class HotStuffConsensus:
    def __init__(self, replicas, id):
        self.replicas = replicas
        self.id = id
        self.view = 0
        self.vheight = 0  # Highest voted height
        self.bexec = None  # Last executed block
        self.block = None  # Locked block
        self.qc_high = None  # Highest QC seen
    
    def on_propose(self, block, qc):
        """Receive proposal from leader"""
        if not self.is_safe(block, qc):
            return
        
        # Vote for block
        vote = Vote(
            view=self.view,
            block=block.hash,
            replica=self.id
        )
        sign(vote, self.private_key)
        
        send_to_leader(vote)
        
        # Update state
        self.vheight = max(self.vheight, block.height)
        
        # Check for commit
        self.update(qc)
    
    def is_safe(self, block, qc):
        """Safety predicate - can we vote?"""
        # Block extends from qc_high
        return (block.parent == qc.block and
                qc.view >= self.qc_high.view)
    
    def update(self, qc):
        """Update state based on QC"""
        if qc.view > self.qc_high.view:
            self.qc_high = qc
            self.block = qc.block
        
        # Three-chain commit rule
        b1 = qc.block
        b2 = b1.parent
        b3 = b2.parent
        
        if b1.height > b2.height > b3.height and \
           consecutive(b1, b2, b3):
            # Commit b3 and all ancestors
            self.commit(b3)
    
    def commit(self, block):
        """Finalize block"""
        if block.height > self.bexec.height:
            # Commit all blocks from bexec to block
            path = get_path(self.bexec, block)
            for b in path:
                execute(b)
            self.bexec = block
    
    def as_leader(self):
        """Leader proposes new block"""
        # Create block extending from qc_high
        block = Block(
            parent=self.qc_high.block,
            height=self.qc_high.block.height + 1,
            qc=self.qc_high,
            transactions=select_transactions()
        )
        
        # Broadcast proposal
        for replica in self.replicas:
            send(replica, Proposal(block, self.qc_high))
        
        # Collect votes
        votes = collect_votes(timeout=DELTA)
        
        if len(votes) >= 2*f + 1:
            # Form QC
            qc = QuorumCertificate(
                view=self.view,
                block=block.hash,
                votes=votes
            )
            
            # Continue to next view
            self.view += 1
            self.as_leader()
```

**Message Complexity**:

| Protocol | Messages per Decision |
|----------|----------------------|
| PBFT | O(n¬≤) - all-to-all |
| Tendermint | O(n¬≤) - all-to-all |
| HotStuff | O(n) - all-to-leader |

HotStuff dramatically reduces bandwidth requirements!

---

## 4. C√¥ng th·ª©c to√°n h·ªçc v√† m·∫≠t m√£ h·ªçc

### 4.1. Byzantine Agreement Impossibility (FLP)

**Fischer-Lynch-Paterson Impossibility Result** (1985):

Trong asynchronous system v·ªõi even one faulty process, kh√¥ng t·ªìn t·∫°i deterministic algorithm ƒë·∫£m b·∫£o consensus.

**Formal Statement**:

System: \( n \) processes, at most \( f = 1 \) can crash
Network: Asynchronous (no timing bounds)
Termination: Must eventually decide

**Theorem**: No deterministic algorithm guarantees termination.

**Proof Sketch**:

1. Assume algorithm \( A \) exists satisfying safety + termination
2. Consider initial configuration \( C_0 \) where processes undecided
3. Consider two possible decisions: 0 and 1
4. Must exist "bivalent" configuration (both outcomes possible)
5. Show every bivalent config has bivalent successor
6. Algorithm can remain bivalent forever ‚Üí never terminates

**Implication for BFT**:

BFT protocols circumvent FLP by:
1. **Relaxing asynchrony**: Assume partial synchrony (eventual timing)
2. **Randomization**: Probabilistic termination (Tendermint, HotStuff)
3. **Failure detector**: Timeout-based suspicion

### 4.2. Quorum Intersection Proof

**Theorem**: V·ªõi \( n = 3f + 1 \) nodes v√† quorum size \( Q = 2f + 1 \), any two quorums intersect in at least \( f + 1 \) nodes.

**Proof**:

Let \( Q_1, Q_2 \) be any two quorums.

\[
\begin{align}
|Q_1 \cap Q_2| &= |Q_1| + |Q_2| - |Q_1 \cup Q_2| \\
&\geq |Q_1| + |Q_2| - n \\
&= (2f + 1) + (2f + 1) - (3f + 1) \\
&= 4f + 2 - 3f - 1 \\
&= f + 1
\end{align}
\]

Since at most \( f \) nodes Byzantine:

\[
\text{Honest nodes in intersection} \geq (f + 1) - f = 1
\]

**Guarantee**: At least one honest node in intersection!

**Safety Implication**:

Cannot have conflicting commits:
- Quorum \( Q_1 \) commits value \( v_1 \)
- Quorum \( Q_2 \) commits value \( v_2 \neq v_1 \)

Contradiction: Honest node in intersection voted for both!

### 4.3. PBFT Complexity Analysis

**Message Complexity per Request**:

| Phase | Messages | Total |
|-------|----------|-------|
| REQUEST | 1 (client ‚Üí primary) | 1 |
| PRE-PREPARE | 1 (primary ‚Üí n-1) | n-1 |
| PREPARE | n √ó (n-1) | n(n-1) |
| COMMIT | n √ó (n-1) | n(n-1) |
| REPLY | n (replicas ‚Üí client) | n |

**Total**: \( O(n^2) \) messages

**Computational Complexity**:

Each message verified:
- Digital signature verification: \( O(1) \) per message
- Total verifications: \( O(n^2) \)

**Latency**:

Assuming network delay \( \delta \):

\[
\text{Latency} = 4\delta + \text{processing time}
\]

- PRE-PREPARE: \( \delta \)
- PREPARE: \( \delta \)
- COMMIT: \( \delta \)
- REPLY: \( \delta \)

**Throughput**:

With pipelining:

\[
\text{Throughput} \approx \frac{1}{\text{processing time per request}}
\]

PBFT can process ~10,000 requests/second on modern hardware.

### 4.4. Tendermint Safety Analysis

**Safety Property**: N·∫øu validator set c√≥ total voting power \( V \), v√† Byzantine validators have power \( B < V/3 \), then safety guaranteed.

**Proof Sketch**:

Assume conflicting commits at height \( h \):
- Block \( b_1 \) committed v·ªõi 2/3+ votes
- Block \( b_2 \neq b_1 \) committed v·ªõi 2/3+ votes

Two quorums intersect in at least \( V/3 + 1 \) voting power.

If \( B < V/3 \), then honest voting power in intersection:
\[
\text{Honest in intersection} \geq (V/3 + 1) - B > 1
\]

At least one honest validator voted for both ‚Üí Impossible due to locking!

Contradiction ‚Üí Safety holds.

### 4.5. HotStuff Linear Messages Proof

**Theorem**: HotStuff achieves O(n) message complexity per view.

**Proof**:

In each view:
- Leader broadcasts proposal: \( n-1 \) messages
- Replicas send votes to leader: \( n-1 \) messages
- Leader broadcasts QC: \( n-1 \) messages

**Total**: \( 3(n-1) = O(n) \)

**Comparison**:
- PBFT: All-to-all communication ‚Üí \( n(n-1) = O(n^2) \)
- HotStuff: Leader-based ‚Üí \( 3(n-1) = O(n) \)

**Bandwidth Improvement**:
\[
\text{Reduction Factor} = \frac{n(n-1)}{3(n-1)} = \frac{n}{3}
\]

For \( n = 100 \): 33√ó less bandwidth!

---

## 5. Implementation Insight

### 5.1. PBFT Implementation

```python
import hashlib
import time
from typing import Dict, List, Set, Optional
from dataclasses import dataclass
from enum import Enum

class MessageType(Enum):
    REQUEST = "request"
    PRE_PREPARE = "pre_prepare"
    PREPARE = "prepare"
    COMMIT = "commit"
    REPLY = "reply"

@dataclass
class Message:
    msg_type: MessageType
    view: int
    sequence: int
    digest: str
    replica_id: int
    timestamp: float = None
    
    def __hash__(self):
        return hash((self.msg_type, self.view, self.sequence, 
                    self.digest, self.replica_id))

class PBFTReplica:
    """PBFT consensus replica"""
    
    def __init__(self, replica_id: int, num_replicas: int):
        self.replica_id = replica_id
        self.n = num_replicas
        self.f = (num_replicas - 1) // 3  # Max Byzantine faults
        
        # State
        self.view = 0
        self.sequence = 0
        self.primary_id = 0
        
        # Message logs
        self.pre_prepare_log: Dict[int, Message] = {}
        self.prepare_log: Dict[int, Set[Message]] = {}
        self.commit_log: Dict[int, Set[Message]] = {}
        
        # Checkpoints
        self.last_checkpoint = 0
        self.checkpoint_interval = 100
        
        # Execution state
        self.executed: Set[int] = set()
        self.prepared: Set[int] = set()
        self.committed_local: Set[int] = set()
    
    def is_primary(self) -> bool:
        """Check if this replica is primary for current view"""
        return self.view % self.n == self.replica_id
    
    def compute_digest(self, request: Dict) -> str:
        """Compute message digest"""
        request_str = str(sorted(request.items()))
        return hashlib.sha256(request_str.encode()).hexdigest()
    
    def receive_request(self, request: Dict) -> Optional[Message]:
        """Primary receives client request"""
        if not self.is_primary():
            return None
        
        # Assign sequence number
        self.sequence += 1
        seq = self.sequence
        
        # Compute digest
        digest = self.compute_digest(request)
        
        # Create PRE-PREPARE message
        pre_prepare = Message(
            msg_type=MessageType.PRE_PREPARE,
            view=self.view,
            sequence=seq,
            digest=digest,
            replica_id=self.replica_id,
            timestamp=time.time()
        )
        
        # Log PRE-PREPARE
        self.pre_prepare_log[seq] = pre_prepare
        
        print(f"Replica {self.replica_id} (PRIMARY): "
              f"PRE-PREPARE seq={seq}, digest={digest[:8]}...")
        
        return pre_prepare
    
    def receive_pre_prepare(self, pre_prepare: Message, 
                           request: Dict) -> Optional[Message]:
        """Backup receives PRE-PREPARE from primary"""
        seq = pre_prepare.sequence
        
        # Validate PRE-PREPARE
        if pre_prepare.view != self.view:
            print(f"Replica {self.replica_id}: "
                  f"PRE-PREPARE wrong view (got {pre_prepare.view}, "
                  f"expected {self.view})")
            return None
        
        if seq in self.pre_prepare_log:
            print(f"Replica {self.replica_id}: "
                  f"Duplicate PRE-PREPARE seq={seq}")
            return None
        
        # Verify digest
        expected_digest = self.compute_digest(request)
        if pre_prepare.digest != expected_digest:
            print(f"Replica {self.replica_id}: "
                  f"PRE-PREPARE digest mismatch")
            return None
        
        # Accept PRE-PREPARE
        self.pre_prepare_log[seq] = pre_prepare
        
        # Send PREPARE
        prepare = Message(
            msg_type=MessageType.PREPARE,
            view=self.view,
            sequence=seq,
            digest=pre_prepare.digest,
            replica_id=self.replica_id,
            timestamp=time.time()
        )
        
        # Log own PREPARE
        if seq not in self.prepare_log:
            self.prepare_log[seq] = set()
        self.prepare_log[seq].add(prepare)
        
        print(f"Replica {self.replica_id}: "
              f"PREPARE seq={seq}, digest={pre_prepare.digest[:8]}...")
        
        return prepare
    
    def receive_prepare(self, prepare: Message) -> Optional[Message]:
        """Receive PREPARE from other replicas"""
        seq = prepare.sequence
        
        # Validate
        if prepare.view != self.view:
            return None
        
        # Add to log
        if seq not in self.prepare_log:
            self.prepare_log[seq] = set()
        self.prepare_log[seq].add(prepare)
        
        # Check if prepared (2f matching PREPAREs + PRE-PREPARE)
        if seq not in self.prepared and \
           len(self.prepare_log[seq]) >= 2 * self.f and \
           seq in self.pre_prepare_log:
            
            self.prepared.add(seq)
            
            print(f"Replica {self.replica_id}: "
                  f"PREPARED seq={seq} "
                  f"({len(self.prepare_log[seq])} prepares)")
            
            # Send COMMIT
            commit = Message(
                msg_type=MessageType.COMMIT,
                view=self.view,
                sequence=seq,
                digest=prepare.digest,
                replica_id=self.replica_id,
                timestamp=time.time()
            )
            
            # Log own COMMIT
            if seq not in self.commit_log:
                self.commit_log[seq] = set()
            self.commit_log[seq].add(commit)
            
            print(f"Replica {self.replica_id}: "
                  f"COMMIT seq={seq}")
            
            return commit
        
        return None
    
    def receive_commit(self, commit: Message) -> bool:
        """Receive COMMIT from other replicas"""
        seq = commit.sequence
        
        # Validate
        if commit.view != self.view:
            return False
        
        # Add to log
        if seq not in self.commit_log:
            self.commit_log[seq] = set()
        self.commit_log[seq].add(commit)
        
        # Check if committed-local (2f+1 matching COMMITs)
        if seq not in self.committed_local and \
           seq in self.prepared and \
           len(self.commit_log[seq]) >= 2 * self.f + 1:
            
            self.committed_local.add(seq)
            
            print(f"Replica {self.replica_id}: "
                  f"COMMITTED-LOCAL seq={seq} "
                  f"({len(self.commit_log[seq])} commits)")
            
            # Execute if not already
            if seq not in self.executed:
                self.execute(seq)
                return True
        
        return False
    
    def execute(self, sequence: int):
        """Execute committed request"""
        if sequence in self.executed:
            return
        
        self.executed.add(sequence)
        
        print(f"Replica {self.replica_id}: "
              f"‚úì EXECUTED seq={sequence}")
    
    def get_status(self) -> Dict:
        """Get replica status"""
        return {
            'replica_id': self.replica_id,
            'view': self.view,
            'sequence': self.sequence,
            'is_primary': self.is_primary(),
            'prepared': len(self.prepared),
            'committed': len(self.committed_local),
            'executed': len(self.executed)
        }

# Example usage - Simulating PBFT
if __name__ == "__main__":
    print("=== PBFT Consensus Simulation ===\n")
    
    # Create replicas (4 replicas, f=1)
    num_replicas = 4
    replicas = [PBFTReplica(i, num_replicas) for i in range(num_replicas)]
    
    print(f"Replicas: {num_replicas}")
    print(f"Fault tolerance: f={replicas[0].f}\n")
    
    # Client request
    request = {
        'operation': 'transfer',
        'from': 'Alice',
        'to': 'Bob',
        'amount': 100
    }
    
    print(f"Client request: {request}\n")
    
    # Phase 1: PRE-PREPARE (primary broadcasts)
    print("--- Phase 1: PRE-PREPARE ---")
    pre_prepare = replicas[0].receive_request(request)
    print()
    
    # Phase 2: PREPARE (backups respond)
    print("--- Phase 2: PREPARE ---")
    prepare_messages = []
    for i in range(1, num_replicas):
        prepare = replicas[i].receive_pre_prepare(pre_prepare, request)
        if prepare:
            prepare_messages.append(prepare)
    print()
    
    # Broadcast PREPARE messages to all replicas
    print("--- Broadcasting PREPARE messages ---")
    commit_messages = []
    for replica in replicas:
        for prepare in prepare_messages:
            commit = replica.receive_prepare(prepare)
            if commit:
                commit_messages.append(commit)
    print()
    
    # Phase 3: COMMIT
    print("--- Phase 3: COMMIT ---")
    print("--- Broadcasting COMMIT messages ---")
    for replica in replicas:
        for commit in commit_messages:
            if commit.replica_id != replica.replica_id:
                replica.receive_commit(commit)
    print()
    
    # Show final status
    print("=== Final Status ===")
    for replica in replicas:
        status = replica.get_status()
        print(f"Replica {status['replica_id']}: "
              f"Executed={status['executed']} operations")
    
    print("\n‚úì Consensus achieved!")
```

---

B√†i gi·∫£ng ƒë·∫°t ~13,000 t·ª´. T√¥i s·∫Ω ti·∫øp t·ª•c v·ªõi b√†i cu·ªëi c√πng c·ªßa Chapter 02 v·ªÅ Hybrid v√† Alternative Consensus Mechanisms! üöÄ
